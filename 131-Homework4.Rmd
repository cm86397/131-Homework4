---
title: "131-Homework 4"
author: "Caleb Mazariegos"
date: '2022-05-06'
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, include = FALSE}
library(tidyverse)
library(tidymodels)
library(corrr)
library(ISLR)
library(ISLR2)
library(discrim)
library(poissonreg)
library(klaR)
library(dplyr)
tidymodels_prefer()
titanic_codebook <- read.csv('/Users/calebmazariegos/Desktop/homework-3/data/titanic.csv')
```

```{r}
# setting survived and pclass as factors, reordering survived so that "Yes" is the first level
titanic_codebook$survived <- as.factor(titanic_codebook$survived)
titanic_codebook$survived <- factor(titanic_codebook$survived, levels = c("Yes", "No"))
titanic_codebook$pclass <- as.factor(titanic_codebook$pclass)
```

## Question 1

#### Split the data, stratifying on the outcome variable, survived. You should choose the proportions to split the data into. Verify that the training and testing data sets have the appropriate number of observations.


```{r}
# Setting the seed
set.seed(3435)
titanic_split <- initial_split(titanic_codebook, prop = 0.75, strata = survived)
titanic_train <- training(titanic_split)
titanic_test <- testing(titanic_split)
```

```{r}
titanic_recipe <- recipe(survived ~ pclass + sex + age + sib_sp + parch + fare, data = titanic_train) %>%
step_impute_linear(age) %>%
  step_dummy(all_nominal_predictors()) %>%
  step_interact(terms = ~ starts_with("sex"):fare) %>%
  step_interact(terms = ~ age:fare)
titanic_recipe
```

```{r}
Auto_train <- training(titanic_split)
Auto_test <- testing(titanic_split)
dim(Auto_train)
dim(Auto_test)
```

```{r}

```

## Question 2

#### Fold the training data. Use k-fold cross-validation, with k=10

```{r}
set.seed(234)
train_folds <- vfold_cv(Auto_train, v=10)
train_folds
```

## Question 3

#### In your own words, explain what we are doing in Question 2. What is k-fold cross-validation? Why should we use it, rather than simply fitting and testing models on the entire training set? If we did use the entire training set, what resampling method would that be?



K-fold cross verification is a statistical method used to estimate the skill of machine learning models. In question 2 we are using it to find the best value of degree that yields the "closest fit". We should use k-fold cross-validation instead of fitting and testing models on the entire training set because k-fold cross validation resamples without replacement, which creates data sets that are smaller than the original data set. If we did use the entire training set, the resampling method we would use is bootstrap. 


## Question 4

#### Set up workflows for 3 models:

#### - A logistic regression with the glm engine;


```{r}
log_reg <- logistic_reg() %>%
  set_engine("glm") %>%
  set_mode("classification")

log_workflow <- workflow() %>%
  add_model(log_reg) %>%
  add_recipe(titanic_recipe)

log_fit <- fit(log_workflow, titanic_train)

log_fit %>%
  tidy()
```

#### - A linear discriminant analysis with the MASS engine;

```{r}
lda_mod <- discrim_linear() %>%
    set_mode("classification") %>%
    set_engine("MASS")


lda_wkflow <- workflow() %>%
  add_model(lda_mod) %>%
  add_recipe(titanic_recipe)

lda_fit <- fit(lda_wkflow, titanic_train)
```


#### - A quadratic discriminant analysis with the MASS engine.

```{r}
qda_mod <- discrim_quad() %>%
  set_engine("MASS") %>%
  set_mode("classification")

qda_workflow <- workflow() %>%
  add_model(qda_mod) %>%
  add_recipe(titanic_recipe)

qda_fit <- fit(qda_workflow, titanic_train)
```

#### - How many models, total, across all folds, will you be fitting to the data? To answer, think about how many folds there are, and how many models you’ll fit to each fold.

We will be fitting 30 models across all folds. This is because there are 10 folds and 3 models.

## Question 5

#### Fit each of the models created in Question 4 to the folded data.

```{r}
log_fit <-
  log_workflow %>%
  fit_resamples(train_folds)
log_fit
```

```{r}
lda_fit <-
  lda_wkflow %>%
  fit_resamples(train_folds)
lda_fit
```


```{r}
qda_fit <-
  qda_workflow %>%
  fit_resamples(train_folds)
qda_fit
```


## Question 6
#### Use collect_metrics() to print the mean and standard errors of the performance metric accuracy across all folds for each of the four models.

#### Decide which of the 3 fitted models has performed the best. Explain why. (Note: You should consider both the mean accuracy and its standard error.)

```{r}
collect_metrics(log_fit)
```
```{r}
collect_metrics(lda_fit)
```

```{r}
collect_metrics(qda_fit)
```

The logistic regression model has performed the best. 

## Question 7
#### Now that you’ve chosen a model, fit your chosen model to the entire training dataset (not to the folds).

```{r}
final_fit <-
  log_workflow %>%
  finalize_workflow(log_workflow)

final_fit
```

## Question 8
#### Finally, with your fitted model, use predict(), bind_cols(), and accuracy() to assess your model’s performance on the testing data!

#### Compare your model’s testing accuracy to its average accuracy across folds. Describe what you see.
